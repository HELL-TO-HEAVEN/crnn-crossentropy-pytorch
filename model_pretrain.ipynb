{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = 'mnist_processed_images/training_set/'\n",
    "VAL_IMG_PATH = 'mnist_processed_images/validation_set/'\n",
    "LABELS = 'mnist_processed_images/training_set_values.txt'\n",
    "VAL_LABELS = 'mnist_processed_images/validation_set_values.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 13\n",
    "MAX_LEN = 10\n",
    "\n",
    "EPOCHS = 42\n",
    "BATCH_SIZE = 32\n",
    "PRETRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.Pad((0, 26)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomAffine(10, translate=(0.1,0.1), scale=(0.95,1.1), shear=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "VAL_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Pad((0, 26)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes, dtype=torch.long) \n",
    "    return y[labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_digits(digits):\n",
    "    \n",
    "    cleaned_digits = []\n",
    "    for digit in digits:\n",
    "        digit = digit.replace('.','10')\n",
    "        digit = digit.replace('-', '11')\n",
    "        cleaned_digits.append(int(digit))\n",
    "\n",
    "    cleaned_digits.append(12)\n",
    "    cleaned_digits += [0] * (MAX_LEN - len(cleaned_digits))\n",
    "    \n",
    "    return one_hot_embedding(cleaned_digits, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import math\n",
    "from torch._six import inf\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "            last_epoch = 0\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state.\n",
    "\n",
    "        Arguments:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "class CyclicLR(_LRScheduler):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `step` should be called after a batch has been used for training.\n",
    "\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for each parameter group.\n",
    "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size_up (int): Number of training iterations in the\n",
    "            increasing half of a cycle. Default: 2000\n",
    "        step_size_down (int): Number of training iterations in the\n",
    "            decreasing half of a cycle. If step_size_down is None,\n",
    "            it is set to step_size_up. Default: None\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            If specified, then 'mode' is ignored.\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
    "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
    "            Default: True\n",
    "        base_momentum (float or list): Initial momentum which is the\n",
    "            lower boundary in the cycle for each parameter group.\n",
    "            Default: 0.8\n",
    "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
    "            for each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
    "            The momentum at any cycle is the difference of max_momentum\n",
    "            and some scaling of the amplitude; therefore\n",
    "            base_momentum may not actually be reached depending on\n",
    "            scaling function. Default: 0.9\n",
    "        last_epoch (int): The index of the last batch. This parameter is used when\n",
    "            resuming a training job. Since `step()` should be invoked after each\n",
    "            batch instead of after each epoch, this number represents the total\n",
    "            number of *batches* computed, not the total number of epochs computed.\n",
    "            When last_epoch=-1, the schedule is started from the beginning.\n",
    "            Default: -1\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         train_batch(...)\n",
    "        >>>         scheduler.step()\n",
    "\n",
    "\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 base_lr,\n",
    "                 max_lr,\n",
    "                 step_size_up=2000,\n",
    "                 step_size_down=None,\n",
    "                 mode='triangular',\n",
    "                 gamma=1.,\n",
    "                 scale_fn=None,\n",
    "                 scale_mode='cycle',\n",
    "                 cycle_momentum=True,\n",
    "                 base_momentum=0.8,\n",
    "                 max_momentum=0.9,\n",
    "                 last_epoch=-1):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
    "        if last_epoch == -1:\n",
    "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
    "                group['lr'] = lr\n",
    "\n",
    "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
    "\n",
    "        step_size_up = float(step_size_up)\n",
    "        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
    "        self.total_size = step_size_up + step_size_down\n",
    "        self.step_ratio = step_size_up / self.total_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "            if 'momentum' not in optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['momentum'] = momentum\n",
    "        self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\n",
    "        self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "        super(CyclicLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def _format_param(self, name, optimizer, param):\n",
    "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
    "        if isinstance(param, (list, tuple)):\n",
    "            if len(param) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
    "                    len(optimizer.param_groups), name, len(param)))\n",
    "            return param\n",
    "        else:\n",
    "            return [param] * len(optimizer.param_groups)\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
    "        `self.last_epoch` as the last batch index.\n",
    "\n",
    "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
    "        updating the optimizer's momentum.\n",
    "        \"\"\"\n",
    "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
    "        x = 1. + self.last_epoch / self.total_size - cycle\n",
    "        if x <= self.step_ratio:\n",
    "            scale_factor = x / self.step_ratio\n",
    "        else:\n",
    "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
    "\n",
    "        lrs = []\n",
    "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
    "            base_height = (max_lr - base_lr) * scale_factor\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
    "            lrs.append(lr)\n",
    "\n",
    "        if self.cycle_momentum:\n",
    "            momentums = []\n",
    "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
    "                base_height = (max_momentum - base_momentum) * scale_factor\n",
    "                if self.scale_mode == 'cycle':\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
    "                else:\n",
    "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
    "                momentums.append(momentum)\n",
    "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
    "                param_group['momentum'] = momentum\n",
    "\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.images = self.df.values[:, 0]\n",
    "        self.labels = self.df.values[:, 1]\n",
    "        self.length = len(self.df.index)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]+'.jpg'\n",
    "        image = Image.open(f'{IMG_PATH}{image_path}')\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        label = str(self.labels[index])\n",
    "        label = one_hot_digits(label)\n",
    "\n",
    "        return (image, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDatasetVal(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.images = self.df.values[:, 0]\n",
    "        self.labels = self.df.values[:, 1]\n",
    "        self.length = len(self.df.index)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(f'{VAL_IMG_PATH}{image_path}')\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        label = str(self.labels[index])\n",
    "\n",
    "        return (image, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, MAX_LEN)\n",
    "        self.lstm = nn.LSTM(OUTPUT_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.out = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        target = target.float()\n",
    "        # Activation function?\n",
    "        latent = F.relu(self.backbone(x))\n",
    "        length = self.linear2(self.dropout1(latent))\n",
    "        \n",
    "        inputs = torch.zeros(BATCH_SIZE, 1, OUTPUT_DIM).to(device)\n",
    "        hidden = (latent.unsqueeze(0), torch.zeros(1, BATCH_SIZE, HIDDEN_DIM).to(device))\n",
    "        number = []\n",
    "        \n",
    "        for i in range(MAX_LEN):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            # Residual Connection?\n",
    "            #hidden = (hidden[0]+latent.unsqueeze(0), hidden[1])\n",
    "            digit = self.out(output[:, -1, :])\n",
    "            number.append(digit.unsqueeze(0))\n",
    "            inputs = target[:, i, :].unsqueeze(1)\n",
    "            \n",
    "        return length, torch.cat(number, 0).transpose(0, 1)\n",
    "    \n",
    "    def to_num(self, number):\n",
    "        clean_number = []\n",
    "        for index in number:\n",
    "            if index == 10:\n",
    "                char = '.'\n",
    "            elif index == 11:\n",
    "                char = '-'\n",
    "            else:\n",
    "                char = str(index)\n",
    "            clean_number.append(char)\n",
    "        return ''.join(clean_number)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        latent = F.relu(self.backbone(x))\n",
    "        \n",
    "        inputs = torch.zeros(1, 1, OUTPUT_DIM).to(device)\n",
    "        hidden = (latent.unsqueeze(0), torch.zeros(1, 1, HIDDEN_DIM).to(device))  \n",
    "        number = []\n",
    "        \n",
    "        for i in range(MAX_LEN):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            #hidden = (hidden[0]+latent.unsqueeze(0), hidden[1])\n",
    "            digit_prob = self.out(output[:, -1, :])\n",
    "            index = torch.max(digit_prob, -1)[1][0]\n",
    "            if index == 12:\n",
    "                break\n",
    "                \n",
    "            inputs = torch.zeros((1, 1, OUTPUT_DIM)).to(device)\n",
    "            inputs[0,0,index] = 1\n",
    "            number.append(index.item())\n",
    "        \n",
    "        return self.to_num(number)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_criterion(x, y):\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for i in range(x.size(1)):\n",
    "        loss += CE(x[:, i, :], y[:, i])\n",
    "    return loss\n",
    "\n",
    "aux_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('pretrain/', train=True, download=True, transform=transforms.Compose([transforms.Grayscale(3),transforms.Resize((224,224)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])), batch_size=32, shuffle=True)\n",
    "                                                    \n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# resnet50.fc = nn.Linear(2048, 10)\n",
    "# resnet50 = resnet50.to(device)\n",
    "# optimizer = optim.Adam(resnet50.parameters(), lr=1e-4)\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for x, y in tqdm(pretrain_loader):\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         out = resnet50(x)\n",
    "#         loss = criterion(out, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(resnet50.state_dict(), 'MNIST_pretrain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50.fc = nn.Linear(2048, 10)\n",
    "if PRETRAIN:\n",
    "    resnet50.load_state_dict(torch.load('MNIST_pretrain.pt'))\n",
    "resnet50.fc = nn.Linear(2048, HIDDEN_DIM)\n",
    "model = CRNN(resnet50)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "step_size = int((EPOCHS*234)/4/2)\n",
    "scheduler = CyclicLR(optimizer, 0.001, 0.01, step_size_up=step_size)\n",
    "print(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(LABELS, sep=';')\n",
    "train_dataset = OCRDataset(train_df, transforms=TRANSFORMS)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(VAL_LABELS, sep=';')\n",
    "val_dataset = OCRDatasetVal(val_df, transforms=VAL_TRANSFORMS)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    running_td_loss = 0\n",
    "    running_aux_loss = 0\n",
    "    accuracies = []\n",
    "    sample_labels = 0\n",
    "    sample_yhat = 0\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    for features, target in tqdm(train_dataloader):\n",
    "        features, target = features.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, number = model(features, target)  \n",
    "        \n",
    "        labels = torch.max(target, -1)[1]\n",
    "        \n",
    "        td_loss = TD_criterion(number, labels)\n",
    "        loss = td_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        running_td_loss += td_loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_hat = torch.max(number, -1)[1].cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        sample_labels = labels\n",
    "        sample_yhat = y_hat\n",
    "        \n",
    "        acc = []\n",
    "        for j in range(y_hat.shape[0]):\n",
    "            acc.append((y_hat[j, :] == labels[j, :]).all())\n",
    "            \n",
    "        accuracies.append(np.sum(acc)/BATCH_SIZE)\n",
    "        scheduler.step()\n",
    "    \n",
    "    model = model.eval()\n",
    "    val_acc = []\n",
    "    \n",
    "    for i, (features, label) in enumerate(val_dataloader):\n",
    "        features = features.to(device)\n",
    "        number = model.predict(features)\n",
    "\n",
    "        if label[0] == number:\n",
    "            val_acc.append(True)\n",
    "        else:\n",
    "            val_acc.append(False)\n",
    "    \n",
    "    \n",
    "    print('[Epoch {}] TD_Loss: {:.5f} Accuracy: {:.5f} Val_Accuracy: {:.5f}'.format(epoch, running_td_loss/len(train_dataloader), np.mean(accuracies), np.sum(val_acc)/len(val_acc)))\n",
    "    print(sample_labels, sample_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pretrained2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
